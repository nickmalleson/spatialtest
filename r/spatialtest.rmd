---
title: "Andresen Spatial Test"
author: "Nick Malleson"
date: '`r format(Sys.time(), "%d %B, %Y (%H:%M)")`'
output:
  html_document: 
    toc: yes
    pandoc_args: [
      "+RTS", "-K64m",
      "-RTS"
    ]
  pdf_document:
    fig_crop: no
    highlight: kate
    keep_tex: yes
    latex_engine: xelatex
    number_sections: yes
fontsize: 10pt
---

```{r initialise, echo=FALSE, message=FALSE, warning=FALSE}
setwd('~/research_not_syncd/git_projects/spatialtest/r/')

library(GISTools)
#library(rgeos)    # For things like gIntersects
library(rgdal)     # For reading shapefiles
library(raster)    # For creating regular grids
#library(plyr)     # For counting and aggregating
#library(tmap)     # For thematic maps
#library(classInt) # Jenks natural breaks
#library(png)      # For loading pngs after they have been written
#library(grid)     # so that they can be embedded in knitted documents
#library(spdep)    # For doing the spatial regression, contiguity matrices, etc.
#library(GWmodel)  # For geographically weighted regression (GWR)
#library(MASS)     # For stepwise regression (stepAIC())
#library(pander)   # For printing tables nicely
#library(MVN)      # For testing for multivariate normality (MVN)

```

# Andresen Spatial Test - R Implementation

## TODO

 - Implement the longitudinal version
 
 - Include option to build a regular grid around the points
 
 - Implement the 'expanding cell' version (lots of regular grids of different sizes and error measures at different scales)

## Background

This is a working document that I am using to develop the Spatial Test in R. Ultimately it will become a self contained function / package.

The test works as follows: (from Andresen and Malleson, 2011)

 1. Nominate a base data set (e.g., 1991 assaults) and count, for each area, the number of points that fall within it.
 
 2. From the test data set (e.g., 1996 assaults), randomly sample 85 percent of the points, with replacement. As with the previous step, count the number of points within each area using the sample. This is effectively a bootstrap created by sampling from the test data set.
 
 3. Repeat (2) a number of times (200 is used here).
 
 4. For each area in the test data set, calculate the percentage of crime that has occurred in the area. Use these percentages to generate a 95 percent nonparametric confidence interval by removing the top and bottom 2.5 percent of all counts (5 from the top and 5 from the bottom in this case). The minimum and maximum of the remaining percentages represent the confidence interval. It should be noted that the effect of the sampling procedure will be to reduce the number of observations in the test data set but, using percentages rather than the absolute counts, comparisons between data sets can be made even if the total number of observations are different.
 
 5. Calculate the percentage of points within each area for the base data set and compare this to the confidence interval generated from the test data set. If the base percentage falls within the confidence interval, then the two data sets exhibit a similar proportion of points in the given area. Otherwise they are significantly different.
 
## Configuration

Configure the script here. (Un)comment or edit lines as appropriate to change the data used and the parameters for the test.

```{r config}
# For the basic test:
#datasource <- "../data/basic_test/" # The folder with the shapefiles

# Some London data:
#datasource <- "../data/london_burglary/" 

# Some Vancouver data:
datasource <- "../data/vancouver_all/"

base.layer <- "points1" # THe name of the base dataset
test.layer <- "points2" # THe name of the test dataset
areas.layer <- "areas"  # The polygon boundary file

N <- 100 # The number of iterations in the monte carlo simulation

# multiprocess <- FALSE # Whether to run the simulation using multiple cores
```
 

## Read Data

Start by reading some data.

```{r readData, message=FALSE }

# Read some data

areas <- readOGR(ds = datasource, layer=areas.layer)
base.data <- readOGR(ds = datasource, layer=base.layer)
test.data <- readOGR(ds = datasource, layer=test.layer)


plot(areas)
plot(base.data, col="blue",add=T)
plot(test.data, col="red",add=T)

```

## Run Administrative Test

Run the test by aggregating points to administrative areas. The code does the following:

 1. Count the number of base and test features in each area.
 2. Run the Monte-Carlo simulation (sample points and count the number in each area)
 2. For each area, calculate the percentage of points in it at every Monte-Carlo iteration
 2. Rank the areas by the percentages in ascending order and remove outliers
 2. Calculate the S-index for each area
 2. Calculate global S value
 2. Return results
 
_Note_: This R implementation was originally provided by [Shane Johnson](https://iris.ucl.ac.uk/iris/browse/profile?upi=SJOHN86) - thanks!!

### Aggregate points.

Count the number of base and test features in each area, see how many are within area boundaries and map them to check that they look OK. This is mostly for checking the data look OK, it isn't a requirement for the algorithm (that begins by aggregating data).

```{r A-aggregate}
areas$base <- poly.counts(base.data, areas)
areas$test <- poly.counts(test.data,areas)

# Check that all of the crimes are within an area
if ( sum(areas$base) != length(base.data) ) {
  warning(paste("There are", length(base.data)-sum(areas$base), "/", length(base.data),
                "crimes in the base data are not within an administrative area"))
}
if ( sum(areas$test) != length(test.data) ) {
  warning(paste("There are", length(test.data)-sum(areas$test),"/", length(test.data),
                "crimes in the test data are not within an administrative area"))
}

if ( 0 %in% areas$base | 0 %in% areas$test) {
  warning(paste("There are", length(areas[areas$base==0,]),"/", length(areas),
                "areas without any base crimes and",length(areas[areas$test==0,]),
                "areas without any test crimes."))
}


```

### Define the test 

Definte a function that will run the test. This function expects an area file to aggregate the points to. This will not be a requirement in later versions.
  
```{r defineAreaTest}

#' Run Martin Andresen's Spatial Test
#' 
#' @param basepoints The base point dataset. This is expected to be a SpatialPointsDataFrame, but might work with other types of objects
#' @param testpoints The test point dataset. This is expected to be a SpatialPointsDataFrame, but might work with other types of objects
#' @param areas The areas to aggregate the points to and run the test on. This is expected to be a SpatialPolygonsDataFrame.
#' @param get.monte.carlo If true, then instead of returning the areas, return an array with the full results of all of the monte carlo runs in it. This is useful for debugging, or for plotting the results of each run. The array has N + 3 dimensions where N is the number of monte carlo iterations and the remaining three dimensions give the mean (N + 1), upper limit (N + 2), lower limit (N + 3).
#' @return The \code{areas} dataset with some new columns:
#'  Local.S - The S index for each area (either -1, 0, or 1)
#'   S - The global S index (which will be the same for each area)
#'   S.nozero - Same as 'S', but ignores areas without any crimes in them
#'   mean - The mean points in each area after resampling
#'   conf.upper - the Upper confidence interval for the test
#'   conf.lower - The lower confidence interval for the test
#' @examples
#' spatialtest(base.data, test.data, areas)
spatialtest <- function(basepoints, testpoints, areas, get.monte.carlo=FALSE) {
  
  # Aggregate the points to the area dataset
  areas$base <- poly.counts(base.data, areas)
  areas$test <- poly.counts(test.data,areas)
  
  # Calculate the percentage of points in each area
  areas$baseProp <- 100 * areas$base / sum(areas$base)
  areas$testProp <- 100 * areas$test / sum(areas$test)

  len <- length(areas) # The number of areas
  tot <- sum(areas$base)    # The number of base crimes

  # Store the results of the montecarlo simulations in a new data.frame
  monte.carlo <- areas
  
  # Now run the Monte-Carlo simulation as follows.
  # 1. Sample the test points (remove 15% at random) and remember the sample size
  # 2. Count the number of points within each area (not necessary?)
  # 3. Calculat the proportion of test points (using the sample size from (1))
  # 4  REPEAT 100 TIMES
  # 5. For each area, rank the proportions of test points in each iteration and remove outliers
  # 6. The top and bottom numbers of test points give the confidence intervals.
  # 7. Calculate s-index for each area
  
  XXXX
  
  
  
  
  # Store the results of each iteration in the monte.carlo array.
  # N columns are for the simulations for each run, remaining three columns are for the mean
  # of all runs and the upper and lower confidence intervals.
  monte.carlo <- array(0,c((N + 3),len))
    
  for (i in 1:N) { # (Could do this bit multi-process)
    #resample (NOT SURE HOW THIS WORKS!!)
    mc.samp <- trunc(runif(tot,min = 1,max = (tot + 1))) # Create a random number for each row
    result[2,(1:tot)] <- result[1,mc.samp[1:tot]] # Store sampled data in result[2,] temporarily
    for (j in 1:len) {
      monte.carlo[i,j] <- sum(result[2,] == j) #compute reaggregate to areas
    }
  }
  
  # Calculate the mean and confidence intervals for each area after the simulations
  temp <- rep(0,N)
  for (i in 1:len) {
    monte.carlo[(N + 1),i] <- mean(monte.carlo[c(1:N),i])
    temp[] <- sort(monte.carlo[c(1:N),i])
    monte.carlo[(N + 2),i] <- temp[0.95 * N] #upper
    monte.carlo[(N + 3),i] <- temp[0.05 * N] #lower
  }
  
  # At this point we can just return the full monte carlo results if that's what the caller wants
  if (get.monte.carlo) {
    return(monte.carlo)
  }
  
  # Compute the local S index for each area.
  # 0 if within confidence intervals, -1 or +1 otherwise
  areas@data$Local.S <- ifelse(
    areas$test <= monte.carlo[(N + 3),], -1,
       ifelse( areas$test >= monte.carlo[(N + 2),], 1, 0 )
    )
  
  
  # Compute the global S Index
  # Do this by adding up the number of times that the test data point is within the upper and lower bounds
  # of the base data. (Could also do this using the local index calculated above)
  S <-  sum(
    (areas$test >= monte.carlo[(N + 3),]) & (areas$test <= monte.carlo[(N + 2),])
    ) / len 
  
  # Calculate excluding zero entries
  # Same as above but also include the condition that there has to be a crime in the test data set
  # as well as being within bounds.
  S.nozero <- sum(
    (areas$test >= monte.carlo[N + 3,]) & (areas$test <= monte.carlo[N + 2,]) & (areas$test > 0)
    ) / len 
  
  #print(paste("S index: ",S, ". Excluding zeros: ",S.nozero))
  
  # Store results ready to be returned when this is implemented as a function
  areas@data$S = S # The s index, will be the same for each area (it is global)
  areas@data$S.nozero = S.nozero
  
  areas@data$mean <- monte.carlo[(N+1),1:len]       # Mean points in each area after resampling
  areas@data$conf.upper <- monte.carlo[(N+2),1:len] # Upper confidence interval
  areas@data$conf.lower <- monte.carlo[(N+3),1:len] # Lower confidence interval
  
  return(areas)
}
 
```

### Run the test

Now just call the function to run the test on the data.

```{r A-runTest}

result <- spatialtest(
  basepoints = base.data,
  testpoints = test.data,
  areas = areas)

print(result)

# Write out to shapefile for sanity checking
#writeOGR(result, ds = datasource, layer= "vancouver_test_r", driver = "ESRI Shapefile", overwrite_layer = TRUE)



```

### Plot the results

First plot all of the samples against the original observations.

```{r A-plotSamples}

  # We need to re-run the test to get the original monte carlo observations 
  monte.carlo <- spatialtest(basepoints = base.data, testpoints = test.data, areas = areas, get.monte.carlo = TRUE)
  
  # Plot the mean of the samples against the observed count.
  plot(
    result@data$mean, result$base, xlab = "Resampled Counts", ylab = "Observed Count",
    main="All samples and their mean for each area in the base data set"
  )

  # Plot all of the samples
  for (i in 1:N) { 
    points(monte.carlo[i,],result$base, col = "grey", pch = 16, cex = 0.5)
  }
  # And add the means back in again (otherwise they are hidden by the samples)
  points(result@data$mean, result$base, pch = 16, col = "black", cex = 0.5, )
  
  # Add a legend
  legend(x=0, y=max(result$base), c('Mean','Samples'), pch=16, col=c("black","grey") )

```

Plot the mean and the confidence intervals for each area.

XXXX - why the mean? Should use number of test points instead? (I tried that and often the number of test points was outside of its own confidence interval!!)

```{r A-plotConfInt}

  # Upper confidence interval first
  plot(
    result$conf.upper, result$base, 
    pch = 16, col = "dark grey",cex = 0.5,xlab = "Bootstrap Estimates",ylab = "Observed Count"
    #main="The mean and confidence interval for each area in the base data set"
  )
  
  # Lower confidence interval
  points(result$conf.lower, result$base,pch = 16,col = "dark grey",cex = 0.5)
  
  # Mean
  points(result$mean, result$base, pch = 16,col = "black",cex = 0.5)
  
  legend(x=0, y=max(result$base), 
         c('Mean','Confidence intervals'),  pch=16, col=c("black","grey"), cex=0.8)

```

Plot the mean and the confidence intervals for each area along with the number of crimes in the test data set.

```{r A-plotConfIntWithTest}

  # Upper confidence interval first
  plot(
    result$conf.upper, result$base, 
    pch = 16, col = "dark grey",cex = 0.5,xlab = "Bootstrap Estimates",ylab = "Observed Count"
    #main="The mean and confidence interval for each area in the base data set"
  )
  
  # Lower confidence interval
  points(result$conf.lower, result$base,pch = 16,col = "dark grey",cex = 0.5)
  
  # Mean
  points(result@data$mean, result$base, pch = 16,col = "black",cex = 0.5)
  
  # Test data - blue for within intervals, red outside
  points(
    result$test, result$base, pch = 16, cex = 0.5,
    col=ifelse(result$test<result$conf.upper | result$test>result$conf.lower, "red", "green") 
  )
  
  legend(x=0, y=max(result$base), 
         c('Mean','Confidence intervals', 'Test value within intervals', 'Test value outside intervals'), 
         pch=16, col=c("black","grey","blue", "red"), cex=0.8)

```

### Map the results of the administrative test

```{r A-MapResults}

  # Create the shading scheme:
  shades <- shading(c(0,1), cols = c("#d8b365", "#f5f5f5", "#5ab4ac"))
  # Create the choropleth map, using the shading scheme defined above
  choropleth(result, result$Local.S, shading = shades, axes=T)
  # Add the lengend. For some reason choro.legend doesn't work
  #choro.legend(px=480000, py=5462000, shades )
  legend(x=480000, y=5462000, c("-1", "0", "+1"), col = c("#d8b365", "#f5f5f5", "#5ab4ac"), pch=15)
  
```


## Validate the test

TODO: run on some real data for Vancouver and Leeds, and check that the results are consistent with previous results calculated using the Java version of the test, or manually (e.g. in Excel)

## Run Grid Tests

Run the test again, but this time creating regular grid(s)

### Define the grid tests

Adapt the original spatial test so that if no areas file is provided, the function will automatically create regular grids and run the test on these. The function will still accept a SpatialPolygonsDataFrame, so can be used as before.

Ultimately, three different kinds of behaviour will be implemented depending on what is passed as the `areas` parameter:

 - If a `SpatialPolygonsDataFrame` is provided, then the test will run using that geography (normal behaviour)
 - If no `areas` parameter is provided, then the test will be run on a number of different regular grids of increasing size.
 - If a list is provided, then the test will assume that this provides the number of cells across (`areas[[1]]`) and down (`areas[[2]]`) and will create a sigle regular grid that spans the extent of all of the input points with that many cells in it.

```{r defineGridTest, eval=FALSE}

#' Run Martin Andresen's Spatial Test
#' 
#' @param basepoints The base point dataset. This is expected to be a SpatialPointsDataFrame, but might work with other types of objects
#' @param testpoints The test point dataset. This is expected to be a SpatialPointsDataFrame, but might work with other types of objects
#' @param areas The areas to aggregate the points to and run the test on. The type of this parameter changes the behaviour of the test:
#' - If a `SpatialPolygonsDataFrame` is provided, then the test will run using that geography (normal behaviour)
#' - If no `areas` parameter is provided, then the test will be run on a number of different regular grids of increasing size.
#' - If a list is provided, then the test will assume that this provides the number of cells across (columsm `areas[[1]]`) and down (rows, `areas[[2]]`) and will create a sigle regular grid that spans the extent of all of the input points with that many cells in it.
#' @param get.monte.carlo If true, then instead of returning the areas, return an array with the full results of all of the monte carlo runs in it. This is useful for debugging, or for plotting the results of each run. The array has N + 3 dimensions where N is the number of monte carlo iterations and the remaining three dimensions give the mean (N + 1), upper limit (N + 2), lower limit (N + 3).
#' @return The \code{areas} dataset with some new columns:
#'  Local.S - The S index for each area (either -1, 0, or 1)
#'   S - The global S index (which will be the same for each area)
#'   S.nozero - Same as 'S', but ignores areas without any crimes in them
#'   mean - The mean points in each area after resampling
#'   conf.upper - the Upper confidence interval for the test
#'   conf.lower - The lower confidence interval for the test
#' @examples
#' spatialtest(base.data, test.data, areas)
spatialtest <- function(basepoints, testpoints, areas, get.monte.carlo=FALSE) {
  
  if (is(areas,"SpatialPolygonsDataFrame")) {
    
    # The areas parameter is some polygons, so no need to do anything special.
    # (This if statement could be removed, but is left here for clarity)
    
  } else if (is.list(areas)) {
    
    # The areas parameter is a list that provides the number of cells across and down. 
    # So now make a regular grid with these characteristics
    
    # Make a raster first, then convert this to a SpatialPolygonsDataFrame
    rast <- raster()
    proj4string(rast) <- proj4string(base.data)
    # Calculate extent of all the point (both crime and test data)
    extent(rast) <- extent(base.data + test.data) # (could use gUnion(), but that's slower)
    # Now set the number of rows and columns
    ncol(rast) <- areas[[1]]
    nrow(rast) <- areas[[2]]
    values(rast)<-0 # Set all cell values to 0
    # Finish by converting to a SpatialPolygonsDataFrame and setting the CRS to match the points
    areas = rasterToPolygons(rast)
    
    
  } else if  (is.null(areas)) {
    
    # If areas is null the run the expanding cell algorithm
    XXXX <- 1
    
  } else {
    
    warning("I don't understand the 'areas' parameter")
    return(-1)
    
  }
  

  
  
  # Aggregate the points to the area dataset
  areas$base <- as.integer(poly.counts(base.data, areas))
  areas$test <- as.integer(poly.counts(test.data,areas))

  len <- length(areas) # The number of areas
  tot <- sum(areas$base)    # The number of crimes
  
  # An array to store the results of each monte carlo iteration temporarily.
  # This is in long form (one row per crime). Dim 1=i, 2=sampled data (temp)
  result <- array(0,c(2,tot+1))
  
  # Generate the long form of the data for sampling - one row per crime
  # (actually it's one crime per colum technically, but that doesn't really matter)
  ct <- 0 # counter
  for (i in 1:len) {
    start <- ct + 1
    end <- start + areas$base[i] - 1 # Number of crimes in area i
    # XXXX ERROR HERE
    print(paste(ct," ",start," ", end, " ",i))
    # Sanity check: shouldn't override so current values in result should be 0
    if (FALSE %in% result[1,c(start:end)]==0) {
      warning("Overriding a previous result!! This shouldn't happen")
    }
    result[1,c(start:end)] <- i
    # XXXX ERROR HERE
    ct <- end
  }
  
  # Now run the Monte-Carlo simulation
  
  # Store the results of each iteration in the monte.carlo array.
  # N columns are for the simulations for each run, remaining three columns are for the mean
  # of all runs and the upper and lower confidence intervals.
  monte.carlo <- array(0,c((N + 3),len))
    
  for (i in 1:N) { # (Could do this bit multi-process)
    #resample (NOT SURE HOW THIS WORKS!!)
    mc.samp <- trunc(runif(tot,min = 1,max = (tot + 1))) # Create a random number for each row
    #print(paste("\t",tot," ",mc.samp))
    result[2,(1:tot)] <- result[1,mc.samp[1:tot]] # Store sampled data in result[2,] temporarily
    for (j in 1:len) {
      monte.carlo[i,j] <- sum(result[2,] == j) #compute reaggregate to areas
    }
  }
  
  # Calculate the mean and confidence intervals for each area after the simulations
  temp <- rep(0,N)
  for (i in 1:len) {
    monte.carlo[(N + 1),i] <- mean(monte.carlo[c(1:N),i])
    temp[] <- sort(monte.carlo[c(1:N),i])
    monte.carlo[(N + 2),i] <- temp[0.95 * N] #upper
    monte.carlo[(N + 3),i] <- temp[0.05 * N] #lower
  }
  
  # At this point we can just return the full monte carlo results if that's what the caller wants
  if (get.monte.carlo) {
    return(monte.carlo)
  }
  
  # Compute the local S index for each area.
  # 0 if within confidence intervals, -1 or +1 otherwise
  areas@data$Local.S <- as.integer(ifelse(
    areas$test <= monte.carlo[(N + 3),], -1,
       ifelse( areas$test >= monte.carlo[(N + 2),], 1, 0 )
    ))
  
  
  # Compute the global S Index
  # Do this by adding up the number of times that the test data point is within the upper and lower bounds
  # of the base data. (Could also do this using the local index calculated above)
  S <-  sum(
    (areas$test >= monte.carlo[(N + 3),]) & (areas$test <= monte.carlo[(N + 2),])
    ) / len 
  
  # Calculate excluding zero entries
  # Same as above but also include the condition that there has to be a crime in the test data set
  # as well as being within bounds.
  S.nozero <- sum(
    (areas$test >= monte.carlo[N + 3,]) & (areas$test <= monte.carlo[N + 2,]) & (areas$test > 0)
    ) / len 
  
  print(paste("S index: ",S, ". Excluding zeros: ",S.nozero))
  
  # Store results ready to be returned when this is implemented as a function
  areas@data$S = S # The s index, will be the same for each area (it is global)
  areas@data$S.nozero = S.nozero
  
  areas@data$mean <- monte.carlo[(N+1),1:len]       # Mean points in each area after resampling
  areas@data$conf.upper <- as.integer(monte.carlo[(N+2),1:len]) # Upper confidence interval
  areas@data$conf.lower <- as.integer(monte.carlo[(N+3),1:len]) # Lower confidence interval
  
  return(areas)
}
 
```

XXXX HERE - FUNCTION DOESN'T WORK - RUN FOLLOWING CHUNK AND DEBUG

UPDATE: I think the problem is with the line: end <- start + areas$base[i] - 1

UPDATE2: I have tested with the Vancouver data. R and Java give entirely different results. Maybe this function is calculating the test a different way. I really need to get my head round the resampling on line 489. Also see the explanation on the paper, pg 103 http://www.tandfonline.com/doi/pdf/10.1080/15230406.2014.972456

If there are no crimes in area i then start>end, which is odd. There might be a bug in the original script. Test this by:
 1. run the test using the vancouver data and output the results
 2. run the test using the same data with the Java program
 3. compare the results of the two. If they're different then the R implementation has a bug I think. It might not have shown itself in Shane's data unless the *last* area had no crimes (if other areas have no crimes it isn't a problem - because incorrect values will just override previous ones??)


### Run the Single Grid Test

Run the test and create a single regular grid.

```{r Grid-runTest, eval=FALSE}
areas <- readOGR(ds = datasource, layer=areas.layer)
#base.data <- readOGR(ds = datasource, layer=base.layer)
#test.data <- readOGR(ds = datasource, layer=test.layer)

result2 <- spatialtest(
  basepoints = base.data,
  testpoints = test.data,
  #areas = list(20,30)
  areas = areas # Useful to use second implementation to run the original areas test (debugging)
  )
writeOGR(result2, ds = datasource, layer= "vancouver_test_r", driver = "ESRI Shapefile", overwrite_layer = TRUE)


```


Check that it looked OK by plotting the confidence intervals

_XXXX Make this plotting code into a function so I don't have to keep repeating it_

```{r B-plotConfInt, eval=FALSE}

  # Upper confidence interval first
  plot(
    result2$conf.upper, result2$base, 
    pch = 16, col = "dark grey",cex = 0.5,xlab = "Bootstrap Estimates",ylab = "Observed Count"
    #main="The mean and confidence interval for each area in the base data set"
  )
  
  # Lower confidence interval
  points(result2$conf.lower, result2$base,pch = 16,col = "dark grey",cex = 0.5)
  
  # Mean
  points(result2$mean, result2$base, pch = 16,col = "black",cex = 0.5)
  
  legend(x=0, y=max(result2$base), 
         c('Mean','Confidence intervals'),  pch=16, col=c("black","grey"), cex=0.8)

```

Map the results of the grid test

```{r B-MapResults, eval=FALSE}

  # Create the shading scheme:
  shades <- shading(c(0,1), cols = c("#d8b365", "#f5f5f5", "#5ab4ac"))
  # Create the choropleth map, using the shading scheme defined above
  choropleth(result2, result2$Local.S, shading = shades, border=NULL)
  # Add the lengend. For some reason choro.legend doesn't work
  #choro.legend(px=480000, py=5462000, shades )
  legend(x=480000, y=5462000, c("-1", "0", "+1"), col = c("#d8b365", "#f5f5f5", "#5ab4ac"), pch=15)
  
```

XXXX HERE - finish implementing the grid stuff above, particularly the expanding cell algorithm.


## Extras

I tried to get a multi-process version of the algorithm working but failed. Here's the code anyway.

```{r multiprocess, eval=FALSE}
    # Do the simulations in single thread (normal) mode, or multi-process
 
  if (multiprocess) {
    # http://projects.revolutionanalytics.com/documents/parallelr/parallerrpkgs/
    # http://stackoverflow.com/questions/1395309/how-to-make-r-use-all-processors
    # Also see 'Using The foreach Package - CRAN' 
    library("foreach")
    library("doMC") # Register a parallel backend (for mac) 
    runSim <- function(iteration, crimes) { # Run one simulation
      # i is the simulation iteration number (not really used
      # crimes is a list of number of crimes in each area
      
      len <- length(crimes)      # The number of areas
      tot <- sum(crimes)    # The number of crimes
      res <- array(0,c(2,tot))  # Make a results array

      ct <- 0 # counter
      for (i in 1:len) {
        start <- ct + 1
        end <- start + crimes[i] - 1 # Number of crimes in area i
        res[1,c(start:end)] <- i
        ct <- end
      }

      mc.samp <- trunc(runif(tot, min = 1,max = (tot + 1))) # Create a random number for each row
      res[2,(1:tot)] <- res[1,mc.samp[1:tot]] # Store sampled data in result[2,] temporarily

      mc <- array(0,c(1,len)) # Store the re-aggregated results
      for (j in 1:len) {
        mc[1,j] <- sum(mc.samp[2,] == j) #compute reaggregate to areas
      }
      mc # Return this
    } # myFunc
    
    # XXXX CAN'T GET THIS WORKING!!
    
    #t <- foreach(sim=1:N, .combine='c') %dopar% { trunc(sim) } # foreach
    
    t <- foreach(sim=1:N, crimes=rep(list(areas$base),N), .combine='c') %dopar% {
      runSim(sim, crimes)
    } # foreach
    
    # XXXX complete the array (turn 't' into monte.carlo)
  }
```