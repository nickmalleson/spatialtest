---
title: "Andresen Spatial Test"
author: "Nick Malleson"
date: '`r format(Sys.time(), "%d %B, %Y (%H:%M)")`'
output:
  html_document: 
    toc: yes
    pandoc_args: [
      "+RTS", "-K64m",
      "-RTS"
    ]
  pdf_document:
    fig_crop: no
    highlight: kate
    keep_tex: yes
    latex_engine: xelatex
    number_sections: yes
fontsize: 10pt
---

```{r initialise, echo=FALSE, message=FALSE, warning=FALSE}
setwd('~/research_not_syncd/git_projects/spatialtest/r/')

library(GISTools)
#library(rgeos)    # For things like gIntersects
library(rgdal)    # For reading shapefiles
#library(plyr)     # For counting and aggregating
#library(tmap)     # For thematic maps
#library(classInt) # Jenks natural breaks
#library(png)      # For loading pngs after they have been written
#library(grid)     # so that they can be embedded in knitted documents
#library(spdep)    # For doing the spatial regression, contiguity matrices, etc.
#library(GWmodel)  # For geographically weighted regression (GWR)
#library(MASS)     # For stepwise regression (stepAIC())
#library(pander)   # For printing tables nicely
#library(MVN)      # For testing for multivariate normality (MVN)

```

# Andresen Spatial Test - R Implementation

## TODO

 - Implement the longitudinal version
 
 - Include option to build a regular grid around the points
 
 - Implement the 'expanding cell' version (lots of regular grids of different sizes and error measures at different scales)

## Background

This is a working document that I am using to develop the Spatial Test in R. Ultimately it will become a self contained function / package.

The test works as follows: (from Andresen and Malleson, 2011)

 1. Nominate a base data set (e.g., 1991 assaults) and count, for each area, the number of points that fall within it.
 
 2. From the test data set (e.g., 1996 assaults), randomly sample 85 percent of the points, with replacement. As with the previous step, count the number of points within each area using the sample. This is effectively a bootstrap created by sampling from the test data set.
 
 3. Repeat (2) a number of times (200 is used here).
 
 4. For each area in the test data set, calculate the percentage of crime that has occurred in the area. Use these percentages to generate a 95 percent nonparametric confidence interval by removing the top and bottom 2.5 percent of all counts (5 from the top and 5 from the bottom in this case). The minimum and maximum of the remaining percentages represent the confidence interval. It should be noted that the effect of the sampling procedure will be to reduce the number of observations in the test data set but, using percentages rather than the absolute counts, comparisons between data sets can be made even if the total number of observations are different.
 
 5. Calculate the percentage of points within each area for the base data set and compare this to the confidence interval generated from the test data set. If the base percentage falls within the confidence interval, then the two data sets exhibit a similar proportion of points in the given area. Otherwise they are significantly different.
 
## Configuration

Configure the script here. (Un)comment or edit lines as appropriate to change the data used and the parameters for the test.

```{r config}
# For the basic test:
#datasource <- "../data/basic_test/" # The folder with the shapefiles

# Some London data:
datasource <- "../data/london_burglary/" 

# Some Vancouver data:
datasource <- "../data/vancouver_all/"

base.layer <- "points1" # THe name of the base dataset
test.layer <- "points2" # THe name of the test dataset
areas.layer <- "areas"  # The polygon boundary file

N <- 100 # The number of iterations in the monte carlo simulation

# multiprocess <- FALSE # Whether to run the simulation using multiple cores
```
 

## Read Data

Start by reading some data.

```{r readData, message=FALSE }

# Read some data

areas <- readOGR(ds = datasource, layer=areas.layer)
base.data <- readOGR(ds = datasource, layer=base.layer)
test.data <- readOGR(ds = datasource, layer=test.layer)


plot(areas)
plot(base.data, col="blue",add=T)
plot(test.data, col="red",add=T)

```

## Run Administrative Test

Run the test by aggregating points to administrative areas. The code does the following:

 1. Count the number of base and test features in each area.
 2. Run the Monte-Carlo simulation (sample points and count the number in each area)
 2. For each area, calculate the percentage of points in it at every Monte-Carlo iteration
 2. Rank the areas by the percentages in ascending order and remove outliers
 2. Calculate the S-index for each area
 2. Calculate global S value
 2. Return results
 
_Note_: This R implementation was originally provided by [Shane Johnson](https://iris.ucl.ac.uk/iris/browse/profile?upi=SJOHN86) - thanks!!

### Aggregate points.

Count the number of base and test features in each area, see how many are within area boundaries and map them to check that they look OK. This is mostly for checking the data look OK.

```{r A-aggregate}
areas$base <- poly.counts(base.data, areas)
areas$test <- poly.counts(test.data,areas)

# Check that all of the crimes are within an area
if ( sum(areas$base) != length(base.data) ) {
  warning(paste("There are", length(base.data)-sum(areas$base), "/", length(base.data),
                "crimes in the base data are not within an administrative area"))
}
if ( sum(areas$test) != length(test.data) ) {
  warning(paste("There are", length(test.data)-sum(areas$test),"/", length(test.data),
                "crimes in the test data are not within an administrative area"))
}

```

### Define the test 

Definte a function that will run the test. This function expects an area file to aggregate the points to. This will not be a requirement in later versions.
  
```{r definteAreaTest}

#' Run Martin Andresen's Spatial Test
#' 
#' @param basepoints The base point dataset. This is expected to be a SpatialPointsDataFrame, but might work with other types of objects
#' @param testpoints The test point dataset. This is expected to be a SpatialPointsDataFrame, but might work with other types of objects
#' @param areas The areas to aggregate the points to and run the test on. This is expected to be a SpatialPolygonsDataFrame.
#' @param get.monte.carlo If true, then instead of returning the areas, return an array with the full results of all of the monte carlo runs in it. This is useful for debugging, or for plotting the results of each run. The array has N + 3 dimensions where N is the number of monte carlo iterations and the remaining three dimensions give the mean (N + 1), upper limit (N + 2), lower limit (N + 3).
#' @return The \code{areas} dataset with some new columns:
#'  Local.S - The S index for each area (either -1, 0, or 1)
#'   S - The global S index (which will be the same for each area)
#'   S.nozero - Same as 'S', but ignores areas without any crimes in them
#'   mean - The mean points in each area after resampling
#'   conf.upper - the Upper confidence interval for the test
#'   conf.lower - The lower confidence interval for the test
#' @examples
#' spatialtest(base.data, test.data, areas)
spatialtest <- function(basepoints, testpoints, areas, get.monte.carlo=FALSE) {
  
  # Aggregate the points to the area dataset
  areas$base <- poly.counts(base.data, areas)
  areas$test <- poly.counts(test.data,areas)

  len <- length(areas) # The number of areas
  tot <- sum(areas$base)    # The number of crimes
  
  # An array to store the results of each monte carlo iteration temporarily.
  # This is in long form (one row per crime). Dim 1=i, 2=sampled data (temp)
  result <- array(0,c(2,tot))
  
  # Generate the long form of the data for sampling - one row per crime
  # (actually it's one crime per colum technically, but that doesn't really matter)
  ct <- 0 # counter
  for (i in 1:len) {
    start <- ct + 1
    end <- start + areas$base[i] - 1 # Number of crimes in area i
    result[1,c(start:end)] <- i
    ct <- end
  }
  
  # Now run the Monte-Carlo simulation
  
  # Store the results of each iteration in the monte.carlo array.
  # N columns are for the simulations for each run, remaining three columns are for the mean
  # of all runs and the upper and lower confidence intervals.
  monte.carlo <- array(0,c((N + 3),len))
    
  for (i in 1:N) { # (Could do this bit multi-process)
    #resample (NOT SURE HOW THIS WORKS!!)
    mc.samp <- trunc(runif(tot,min = 1,max = (tot + 1))) # Create a random number for each row
    result[2,(1:tot)] <- result[1,mc.samp[1:tot]] # Store sampled data in result[2,] temporarily
    for (j in 1:len) {
      monte.carlo[i,j] <- sum(result[2,] == j) #compute reaggregate to areas
    }
  }
  
  # Calculate the mean and confidence intervals for each area after the simulations
  temp <- rep(0,N)
  for (i in 1:len) {
    monte.carlo[(N + 1),i] <- mean(monte.carlo[c(1:N),i])
    temp[] <- sort(monte.carlo[c(1:N),i])
    monte.carlo[(N + 2),i] <- temp[0.95 * N] #upper
    monte.carlo[(N + 3),i] <- temp[0.05 * N] #lower
  }
  
  # At this point we can just return the full monte carlo results if that's what the caller wants
  if (get.monte.carlo) {
    return(monte.carlo)
  }
  
  # Compute the local S index for each area.
  # 0 if within confidence intervals, -1 or +1 otherwise
  areas@data$Local.S <- ifelse(
    areas$test <= monte.carlo[(N + 3),], -1,
       ifelse( areas$test >= monte.carlo[(N + 2),], 1, 0 )
    )
  
  
  # Compute the global S Index
  # Do this by adding up the number of times that the test data point is within the upper and lower bounds
  # of the base data. (Could also do this using the local index calculated above)
  S <-  sum(
    (areas$test >= monte.carlo[(N + 3),]) & (areas$test <= monte.carlo[(N + 2),])
    ) / len 
  
  # Calculate excluding zero entries
  # Same as above but also include the condition that there has to be a crime in the test data set
  # as well as being within bounds.
  S.nozero <- sum(
    (areas$test >= monte.carlo[N + 3,]) & (areas$test <= monte.carlo[N + 2,]) & (areas$test > 0)
    ) / len 
  
  print(paste("S index: ",S, ". Excluding zeros: ",S.nozero))
  
  # Store results ready to be returned when this is implemented as a function
  areas@data$S = S # The s index, will be the same for each area (it is global)
  areas@data$S.nozero = S.nozero
  
  areas@data$mean <- monte.carlo[(N+1),1:len]       # Mean points in each area after resampling
  areas@data$conf.upper <- monte.carlo[(N+2),1:len] # Upper confidence interval
  areas@data$conf.lower <- monte.carlo[(N+3),1:len] # Lower confidence interval
  
  return(areas)
}
 
```

### Run the test

Now just call the function to run the test on the data.

```{r A-runTest}

result <- spatialtest(
  basepoints = base.data,
  testpoints = test.data,
  areas = areas)

```

### Plot the results

First plot all of the samples against the original observations.

```{r A-plotSamples}

  # We need to re-run the test to get the original monte carlo observations 
  monte.carlo <- spatialtest(basepoints = base.data, testpoints = test.data, areas = areas, get.monte.carlo = TRUE)
  
  # Plot the mean of the samples against the observed count.
  plot(
    result@data$mean, result$base, xlab = "Resampled Counts", ylab = "Observed Count",
    main="All samples and their mean for each area in the base data set"
  )

  # Plot all of the samples
  for (i in 1:N) { 
    points(monte.carlo[i,],result$base, col = "grey", pch = 16, cex = 0.5)
  }
  # And add the means back in again (otherwise they are hidden by the samples)
  points(result@data$mean, result$base, pch = 16, col = "black", cex = 0.5, )
  
  # Add a legend
  legend(x=0, y=max(result$base), c('Mean','Samples'), pch=16, col=c("black","grey") )

```

Plot the mean and the confidence intervals for each area

```{r A-plotConfInt}

  # Upper confidence interval first
  plot(
    result$conf.upper, result$base, 
    pch = 16, col = "dark grey",cex = 0.5,xlab = "Bootstrap Estimates",ylab = "Observed Count"
    #main="The mean and confidence interval for each area in the base data set"
  )
  
  # Lower confidence interval
  points(result$conf.lower, result$base,pch = 16,col = "dark grey",cex = 0.5)
  
  # Mean
  points(result$mean, result$base, pch = 16,col = "black",cex = 0.5)
  
  legend(x=0, y=max(result$base), 
         c('Mean','Confidence intervals'),  pch=16, col=c("black","grey"), cex=0.8)

```

Plot the mean and the confidence intervals for each area along with the number of crimes in the test data set.

```{r A-plotConfIntWithTest}

  # Upper confidence interval first
  plot(
    result$conf.upper, result$base, 
    pch = 16, col = "dark grey",cex = 0.5,xlab = "Bootstrap Estimates",ylab = "Observed Count"
    #main="The mean and confidence interval for each area in the base data set"
  )
  
  # Lower confidence interval
  points(result$conf.lower, result$base,pch = 16,col = "dark grey",cex = 0.5)
  
  # Mean
  points(result@data$mean, result$base, pch = 16,col = "black",cex = 0.5)
  
  # Test data - blue for within intervals, red outside
  points(
    result$test, result$base, pch = 16, cex = 0.5,
    col=ifelse(result$test<result$conf.upper | result$test>result$conf.lower, "red", "green") 
  )
  
  legend(x=0, y=max(result$base), 
         c('Mean','Confidence intervals', 'Test value within intervals', 'Test value outside intervals'), 
         pch=16, col=c("black","grey","blue", "green"), cex=0.8)

```

### Map the results of the administrative test

```{r A-MapResults}

  # Create the shading scheme:
  shades <- shading(c(0,1), cols = c("#d8b365", "#f5f5f5", "#5ab4ac"))
  # Create the choropleth map, using the shading scheme defined above
  choropleth(result, result$Local.S, shading = shades)
  # Add the lengend. For some reason choro.legend doesn't work
  #choro.legend(px=480000, py=5462000, shades )
  legend(x=480000, y=5462000, c("-1", "0", "+1"), col = c("#d8b365", "#f5f5f5", "#5ab4ac"), pch=15)
  
```


## Validate the test

TODO: run on some real data for Vancouver and Leeds, and check that the results are consistent with previous results calculated using the Java version of the test, or manually (e.g. in Excel)

## Run Grid Test

Run the test again, but this time create a regular grid

XXXX - first wrap the test above in a function

XXXX

## Run Expanding Cell test


## Extras

I tried to get a multi-process version of the algorithm working but failed. Here's the code anyway.

```{r multiprocess, eval=FALSE}
    # Do the simulations in single thread (normal) mode, or multi-process
 
  if (multiprocess) {
    # http://projects.revolutionanalytics.com/documents/parallelr/parallerrpkgs/
    # http://stackoverflow.com/questions/1395309/how-to-make-r-use-all-processors
    # Also see 'Using The foreach Package - CRAN' 
    library("foreach")
    library("doMC") # Register a parallel backend (for mac) 
    runSim <- function(iteration, crimes) { # Run one simulation
      # i is the simulation iteration number (not really used
      # crimes is a list of number of crimes in each area
      
      len <- length(crimes)      # The number of areas
      tot <- sum(crimes)    # The number of crimes
      res <- array(0,c(2,tot))  # Make a results array

      ct <- 0 # counter
      for (i in 1:len) {
        start <- ct + 1
        end <- start + crimes[i] - 1 # Number of crimes in area i
        res[1,c(start:end)] <- i
        ct <- end
      }

      mc.samp <- trunc(runif(tot, min = 1,max = (tot + 1))) # Create a random number for each row
      res[2,(1:tot)] <- res[1,mc.samp[1:tot]] # Store sampled data in result[2,] temporarily

      mc <- array(0,c(1,len)) # Store the re-aggregated results
      for (j in 1:len) {
        mc[1,j] <- sum(mc.samp[2,] == j) #compute reaggregate to areas
      }
      mc # Return this
    } # myFunc
    
    # XXXX CAN'T GET THIS WORKING!!
    
    #t <- foreach(sim=1:N, .combine='c') %dopar% { trunc(sim) } # foreach
    
    t <- foreach(sim=1:N, crimes=rep(list(areas$base),N), .combine='c') %dopar% {
      runSim(sim, crimes)
    } # foreach
    
    # XXXX complete the array (turn 't' into monte.carlo)
  }
```