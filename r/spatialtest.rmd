---
title: "Andresen Spatial Test"
author: "Nick Malleson"
date: '`r format(Sys.time(), "%d %B, %Y (%H:%M)")`'
output:
  html_document: 
    toc: yes
    pandoc_args: [
      "+RTS", "-K64m",
      "-RTS"
    ]
  pdf_document:
    fig_crop: no
    highlight: kate
    keep_tex: yes
    latex_engine: xelatex
    number_sections: yes
fontsize: 10pt
---

```{r initialise, echo=FALSE, message=FALSE, warning=FALSE}
setwd('~/research_not_syncd/git_projects/spatialtest/r/')

library(GISTools)
#library(rgeos)    # For things like gIntersects
library(rgdal)    # For reading shapefiles
#library(plyr)     # For counting and aggregating
#library(tmap)     # For thematic maps
#library(classInt) # Jenks natural breaks
#library(png)      # For loading pngs after they have been written
#library(grid)     # so that they can be embedded in knitted documents
#library(spdep)    # For doing the spatial regression, contiguity matrices, etc.
#library(GWmodel)  # For geographically weighted regression (GWR)
#library(MASS)     # For stepwise regression (stepAIC())
#library(pander)   # For printing tables nicely
#library(MVN)      # For testing for multivariate normality (MVN)

```

# Andresen Spatial Test - R Implementation

## TODO

 - Implement the longitudinal version
 
 - Include option to build a regular grid around the points
 
 - Implement the 'expanding cell' version (lots of regular grids of different sizes and error measures at different scales)

## Background

This is a working document that I am using to develop the Spatial Test in R. Ultimately it will become a self contained function / package.

The test works as follows: (from Andresen and Malleson, 2011)

 1. Nominate a base data set (e.g., 1991 assaults) and count, for each area, the number of points that fall within it.
 
 2. From the test data set (e.g., 1996 assaults), randomly sample 85 percent of the points, with replacement. As with the previous step, count the number of points within each area using the sample. This is effectively a bootstrap created by sampling from the test data set.
 
 3. Repeat (2) a number of times (200 is used here).
 
 4. For each area in the test data set, calculate the percentage of crime that has occurred in the area. Use these percentages to generate a 95 percent nonparametric confidence interval by removing the top and bottom 2.5 percent of all counts (5 from the top and 5 from the bottom in this case). The minimum and maximum of the remaining percentages represent the confidence interval. It should be noted that the effect of the sampling procedure will be to reduce the number of observations in the test data set but, using percentages rather than the absolute counts, comparisons between data sets can be made even if the total number of observations are different.
 
 5. Calculate the percentage of points within each area for the base data set and compare this to the confidence interval generated from the test data set. If the base percentage falls within the confidence interval, then the two data sets exhibit a similar proportion of points in the given area. Otherwise they are significantly different.
 
## Configuration

Configure the script here. (Un)comment or edit lines as appropriate to change the data used and the parameters for the test.

```{r config}
# For the basic test:
#datasource <- "../data/basic_test/" # The folder with the shapefiles

# Some London data:
datasource <- "../data/london_burglary/" 

# Some Vancouver data:
datasource <- "../data/vancouver_all/"

base.layer <- "points1" # THe name of the base dataset
test.layer <- "points2" # THe name of the test dataset
areas.layer <- "areas"  # The polygon boundary file

N <- 100 # The number of iterations in the monte carlo simulation

# multiprocess <- FALSE # Whether to run the simulation using multiple cores
```
 

## Read Data

Start by reading some data.

```{r readData, message=FALSE }

# Read some data

areas <- readOGR(ds = datasource, layer=areas.layer)
base.data <- readOGR(ds = datasource, layer=base.layer)
test.data <- readOGR(ds = datasource, layer=test.layer)


plot(areas)
plot(base.data, col="blue",add=T)
plot(test.data, col="red",add=T)

```

## Run Administrative Test

Run the test by aggregating points to administrative areas. The code does the following:

 1. Count the number of base and test features in each area.
 2. Run the Monte-Carlo simulation (sample points and count the number in each area)
 2. For each area, calculate the percentage of points in it at every Monte-Carlo iteration
 2. Rank the areas by the percentages in ascending order and remove outliers
 2. Calculate the S-index for each area
 2. Calculate global S value
 2. Return results
 
_Note_: This R implementation was originally provided by Shane Johnson - thanks!!

### Aggregate points.

```{r aggregate}
# Count the number of base and test features in each area
areas$base <- poly.counts(base.data, areas)
areas$test <- poly.counts(test.data,areas)

# Check that all of the crimes are within an area
if ( sum(areas$base) != length(base.data) ) {
  warning(paste("There are", length(base.data)-sum(areas$base), "/", length(base.data),
                "crimes in the base data are not within an administrative area"))
}
if ( sum(areas$test) != length(test.data) ) {
  warning(paste("There are", length(test.data)-sum(areas$test),"/", length(test.data),
                "crimes in the test data are not within an administrative area"))
}

```

### Run the test 
  
```{r runTest}

  len <- length(areas) # The number of areas
  tot <- sum(areas$base)    # The number of crimes
  
  # An array to store the results of each monte carlo iteration temporarily.
  # This is in long form (one row per crime). Dim 1=i, 2=sampled data (temp)
  result <- array(0,c(2,tot))
  
  # Generate the long form of the data for sampling - one row per crime
  # (actually it's one crime per colum technically, but that doesn't really matter)
  ct <- 0 # counter
  for (i in 1:len) {
    start <- ct + 1
    end <- start + areas$base[i] - 1 # Number of crimes in area i
    result[1,c(start:end)] <- i
    ct <- end
  }
  
  # Now run the Monte-Carlo simulation
  
  # Store the results of each iteration in the monte.carlo array.
  # N columns are for the simulations for each run, remaining three columns are for the mean
  # of all runs and the upper and lower confidence intervals.
  monte.carlo <- array(0,c((N + 3),len))
    
  for (i in 1:N) { # (Could do this bit multi-process)
    #resample (NOT SURE HOW THIS WORKS!!)
    mc.samp <- trunc(runif(tot,min = 1,max = (tot + 1))) # Create a random number for each row
    result[2,(1:tot)] <- result[1,mc.samp[1:tot]] # Store sampled data in result[2,] temporarily
    for (j in 1:len) {
      monte.carlo[i,j] <- sum(result[2,] == j) #compute reaggregate to areas
    }
  }
  
  # Calculate the mean and confidence intervals for each area after the simulations
  temp <- rep(0,N)
  for (i in 1:len) {
    monte.carlo[(N + 1),i] <- mean(monte.carlo[c(1:N),i])
    temp[] <- sort(monte.carlo[c(1:N),i])
    monte.carlo[(N + 2),i] <- temp[0.95 * N] #upper
    monte.carlo[(N + 3),i] <- temp[0.05 * N] #lower
  }
  
  # Compute the S Index, in this case comparing the values from year to the next for each area
  # Do this by adding up the number of times that the test data point is within the upper and lower bounds
  # of the base data
  S <-  sum(
    (areas$test >= monte.carlo[(N + 3),]) & (areas$test <= monte.carlo[(N + 2),])
    ) / len 
  
  # Calculate excluding zero entries
  # Same as above but also include the condition that there has to be a crime in the test data set
  # as well as being within bounds.
  S.nozero <- sum(
    (areas$test >= monte.carlo[N + 3,]) & (areas$test <= monte.carlo[N + 2,]) & (areas$test > 0)
    ) / len 
  
  print(paste("S index: ",S, ". Excluding zeros: ",S.nozero))
  
  # Store results ready to be returned when this is implemented as a function
  areas@data$S = S # The s index, will be the same for each area (it is global)
  areas@data$S.nozero = S.nozero
  
  areas@data$mean <- monte.carlo[(N+1),1:len]       # Mean points in each area after resampling
  areas@data$conf.upper <- monte.carlo[(N+2),1:len] # Lower confidence interval
  areas@data$conf.lower <- monte.carlo[(N+3),1:len] # Upper confidence interval
 
```

### Plot the results

```{r plot}
  
  plot(
    monte.carlo[(N + 1),],areas$base,
    pch = 16, col = "black", cex = 0.5, xlab = "Resampled Counts", ylab = "Observed Count"
  )

  for (i in 1:N) {
    points(monte.carlo[i,],areas$base, col = "grey", pch = 16, cex = 0.5)
  }
  plot(
    monte.carlo[(N + 2),], areas$base[], 
    pch = 16,col = "dark grey",cex = 0.5,xlab = "Bootstrap Estimates",ylab = "Observed Count"
  )
  
  points(monte.carlo[(N + 3),], areas$base,pch = 16,col = "dark grey",cex = 0.5)	#Just to check things out, plot the actual and bootstrapped values	against each other
  points(monte.carlo[(N + 1),], areas$base,pch = 16,col = "black",cex = 0.5)
  text(125,30,"Mean",pos = 4,cex = 0.75)
  text(125,15,"Confidence Intervals",pos = 4,cex = 0.75)
  points(125,31,col = "black",pch = 16,cex = 0.8)
  points(125,16,col = "grey",pch = 16,cex = 0.8)
  




#t <- andresen.spatial.test(base=base.data, test=test.data, areas=areas)


```

## Validate the test

TODO: run on some real data for Vancouver and Leeds, and check that the results are consistent with previous results calculated using the Java version of the test, or manually (e.g. in Excel)



## Extras

I tried to get a multi-process version of the algorithm working but failed. Here's the code anyway.

```{r multiprocess, eval=FALSE}
    # Do the simulations in single thread (normal) mode, or multi-process
 
  if (multiprocess) {
    # http://projects.revolutionanalytics.com/documents/parallelr/parallerrpkgs/
    # http://stackoverflow.com/questions/1395309/how-to-make-r-use-all-processors
    # Also see 'Using The foreach Package - CRAN' 
    library("foreach")
    library("doMC") # Register a parallel backend (for mac) 
    runSim <- function(iteration, crimes) { # Run one simulation
      # i is the simulation iteration number (not really used
      # crimes is a list of number of crimes in each area
      
      len <- length(crimes)      # The number of areas
      tot <- sum(crimes)    # The number of crimes
      res <- array(0,c(2,tot))  # Make a results array

      ct <- 0 # counter
      for (i in 1:len) {
        start <- ct + 1
        end <- start + crimes[i] - 1 # Number of crimes in area i
        res[1,c(start:end)] <- i
        ct <- end
      }

      mc.samp <- trunc(runif(tot, min = 1,max = (tot + 1))) # Create a random number for each row
      res[2,(1:tot)] <- res[1,mc.samp[1:tot]] # Store sampled data in result[2,] temporarily

      mc <- array(0,c(1,len)) # Store the re-aggregated results
      for (j in 1:len) {
        mc[1,j] <- sum(mc.samp[2,] == j) #compute reaggregate to areas
      }
      mc # Return this
    } # myFunc
    
    # XXXX CAN'T GET THIS WORKING!!
    
    #t <- foreach(sim=1:N, .combine='c') %dopar% { trunc(sim) } # foreach
    
    t <- foreach(sim=1:N, crimes=rep(list(areas$base),N), .combine='c') %dopar% {
      runSim(sim, crimes)
    } # foreach
    
    # XXXX complete the array (turn 't' into monte.carlo)
  }
```