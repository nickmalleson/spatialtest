version()
v()
Version()
version
d <- read.csv('mapping/writing/sttc/twitter_data/2/satscan/tweets_cases-hours.csv')
d
names(d)
hist(d$Count)
hist(d$Count, breaks="Scott")
load("/Volumes/a204/geonsm/mapping/writing/ambient_crime3/ambientcrime3.RData")
cpd.ambient.aggregate <- aggregate(Total ~ Grid.ID, data=cpd.ambient, mean)
cpd <- merge(x=lon.grid, y=cpd.ambient.aggregate, by.x="ID", by.y="Grid.ID", all.x=TRUE)
plot(cpd)
save.image("/Volumes/a204/geonsm/mapping/writing/ambient_crime3/ambientcrime3.RData")
head(cpd, 10)
cpd.ambient.aggregate <- aggregate(Total ~ Grid.ID, data=cpd.ambient, mean)
cpd <- merge(x=lon.grid, y=cpd.ambient.aggregate, by.x="ID", by.y="Grid.ID", all.x=TRUE)
save.image("/Volumes/a204/geonsm/mapping/writing/ambient_crime3/ambientcrime3.RData")
plotGoogleMaps(lon.wz, zcol=lon.wz@data$people,filename="temp.html",layerName="Usual Workplace Population", fillOpacity=0.4,strokeWeight=0,mapTypeId="ROADMAP")
library(plotGoogleMaps)
plotGoogleMaps(lon.wz, zcol=lon.wz@data$people,filename="temp.html",layerName="Usual Workplace Population", fillOpacity=0.4,strokeWeight=0,mapTypeId="ROADMAP")
?plotGoogleMaps
plotGoogleMaps(lon.wz, zcol="people" ,filename="temp.html",layerName="Usual Workplace Population", fillOpacity=0.4,strokeWeight=0,mapTypeId="ROADMAP")
library(tmap)
install.packages(tmap)
install.packages("tmap")
vignette(package = "tmap") # available vignettes in tmap
vignette("tmap-nutshell")
qtm(shp = cpd, fill = "people", fill.palette = "-Blues")
library(tmap)
qtm(shp = cpd, fill = "people", fill.palette = "-Blues")
/Crime da
setwd('~/research_not_syncd/git_projects/spatialtest/r/')
library(GISTools)
#library(rgeos)    # For things like gIntersects
library(rgdal)    # For reading shapefiles
#library(plyr)     # For counting and aggregating
#library(tmap)     # For thematic maps
#library(classInt) # Jenks natural breaks
#library(png)      # For loading pngs after they have been written
#library(grid)     # so that they can be embedded in knitted documents
#library(spdep)    # For doing the spatial regression, contiguity matrices, etc.
#library(GWmodel)  # For geographically weighted regression (GWR)
#library(MASS)     # For stepwise regression (stepAIC())
#library(pander)   # For printing tables nicely
#library(MVN)      # For testing for multivariate normality (MVN)
# For the basic test:
#datasource <- "../data/basic_test/" # The folder with the shapefiles
# Some London data:
datasource <- "../data/london_burglary/"
# Some Vancouver data:
#XXXX
base.layer <- "points1" # THe name of the base dataset
test.layer <- "points2" # THe name of the test dataset
areas.layer <- "areas"  # The polygon boundary file
N <- 100 # The number of iterations in the monte carlo simulation
multiprocess <- FALSE # Whether to run the simulation using multiple cores
setwd('~/research_not_syncd/git_projects/spatialtest/r/')
library(GISTools)
#library(rgeos)    # For things like gIntersects
library(rgdal)    # For reading shapefiles
#library(plyr)     # For counting and aggregating
#library(tmap)     # For thematic maps
#library(classInt) # Jenks natural breaks
#library(png)      # For loading pngs after they have been written
#library(grid)     # so that they can be embedded in knitted documents
#library(spdep)    # For doing the spatial regression, contiguity matrices, etc.
#library(GWmodel)  # For geographically weighted regression (GWR)
#library(MASS)     # For stepwise regression (stepAIC())
#library(pander)   # For printing tables nicely
#library(MVN)      # For testing for multivariate normality (MVN)
# For the basic test:
#datasource <- "../data/basic_test/" # The folder with the shapefiles
# Some London data:
datasource <- "../data/london_burglary/"
# Some Vancouver data:
datasource <- "../data/vancouver_all/"
base.layer <- "points1" # THe name of the base dataset
test.layer <- "points2" # THe name of the test dataset
areas.layer <- "areas"  # The polygon boundary file
N <- 100 # The number of iterations in the monte carlo simulation
# multiprocess <- FALSE # Whether to run the simulation using multiple cores
# Read some data
areas <- readOGR(ds = datasource, layer=areas.layer)
base.data <- readOGR(ds = datasource, layer=base.layer)
test.data <- readOGR(ds = datasource, layer=test.layer)
plot(areas)
plot(base.data, col="blue",add=T)
plot(test.data, col="red",add=T)
plot(areas)
plot(base.data, col="blue",add=T)
plot(test.data, col="red",add=T)
plot(test.data, col="red")
plot(areas)
plot(test.data, col="red")
plot(areas, add=T)
CRS(areas)
CRS(test.data)
# Count the number of base and test features in each area
areas$base <- poly.counts(base.data, areas)
areas$test <- poly.counts(test.data,areas)
# Check that all of the crimes are within an area
if ( sum(areas$base) != length(base.data) ) {
warning(paste("There are", length(base.data)-sum(areas$base),
"crimes in the base data are not within an administrative area"))
}
if ( sum(areas$test) != length(test.data) ) {
warning(paste("There are", length(test.data)-sum(areas$test),
"crimes in the test data are not within an administrative area"))
}
# Count the number of base and test features in each area
areas$base <- poly.counts(base.data, areas)
areas$test <- poly.counts(test.data,areas)
# Check that all of the crimes are within an area
if ( sum(areas$base) != length(base.data) ) {
warning(paste("There are", length(base.data)-sum(areas$base), "/", length(base.data),
"crimes in the base data are not within an administrative area"))
}
if ( sum(areas$test) != length(test.data) ) {
warning(paste("There are", length(test.data)-sum(areas$test),"/", length(test.data),
"crimes in the test data are not within an administrative area"))
}
# Read some data
areas <- readOGR(ds = datasource, layer=areas.layer)
base.data <- readOGR(ds = datasource, layer=base.layer)
test.data <- readOGR(ds = datasource, layer=test.layer)
plot(areas)
plot(base.data, col="blue",add=T)
plot(test.data, col="red",add=T)
# Count the number of base and test features in each area
areas$base <- poly.counts(base.data, areas)
areas$test <- poly.counts(test.data,areas)
# Check that all of the crimes are within an area
if ( sum(areas$base) != length(base.data) ) {
warning(paste("There are", length(base.data)-sum(areas$base), "/", length(base.data),
"crimes in the base data are not within an administrative area"))
}
if ( sum(areas$test) != length(test.data) ) {
warning(paste("There are", length(test.data)-sum(areas$test),"/", length(test.data),
"crimes in the test data are not within an administrative area"))
}
len <- length(areas) # The number of areas
tot <- sum(areas$base)    # The number of crimes
# An array to store the results of each monte carlo iteration temporarily.
# This is in long form (one row per crime). Dim 1=i, 2=sampled data (temp)
result <- array(0,c(2,tot))
# Generate the long form of the data for sampling - one row per crime
# (actually it's one crime per colum technically, but that doesn't really matter)
ct <- 0 # counter
for (i in 1:len) {
start <- ct + 1
end <- start + areas$base[i] - 1 # Number of crimes in area i
result[1,c(start:end)] <- i
ct <- end
}
# Now run the Monte-Carlo simulation
# Store the results of each iteration in the monte.carlo array.
# N columns are for the simulations for each run, remaining three columns are for the mean
# of all runs and the upper and lower confidence intervals.
monte.carlo <- array(0,c((N + 3),len))
for (i in 1:N) { # (Could do this bit multi-process)
#resample (NOT SURE HOW THIS WORKS!!)
mc.samp <- trunc(runif(tot,min = 1,max = (tot + 1))) # Create a random number for each row
result[2,(1:tot)] <- result[1,mc.samp[1:tot]] # Store sampled data in result[2,] temporarily
for (j in 1:len) {
monte.carlo[i,j] <- sum(result[2,] == j) #compute reaggregate to areas
}
}
# Calculate the mean and confidence intervals for each area after the simulations
temp <- rep(0,N)
for (i in 1:len) {
monte.carlo[(N + 1),i] <- mean(monte.carlo[c(1:N),i])
temp[] <- sort(monte.carlo[c(1:N),i])
monte.carlo[(N + 2),i] <- temp[0.95 * N] #upper
monte.carlo[(N + 3),i] <- temp[0.05 * N] #lower
}
# Compute the S Index, in this case comparing the values from year to the next for each area
# Do this by adding up the number of times that the test data point is within the upper and lower bounds
# of the base data
S <-  sum(
(areas$test >= monte.carlo[(N + 3),]) & (areas$test <= monte.carlo[(N + 2),])
) / len
# Calculate excluding zero entries
# Same as above but also include the condition that there has to be a crime in the test data set
# as well as being within bounds.
S.nozero <- sum(
(areas$test >= monte.carlo[N + 3,]) & (areas$test <= monte.carlo[N + 2,]) & (areas$test > 0)
) / len
print(paste("S index: ",S, ". Excluding zeros: ",S.nozero))
# Store results ready to be returned when this is implemented as a function
areas@data$S = S # The s index, will be the same for each area (it is global)
areas@data$S.nozero = S.nozero
areas@data$mean <- monte.carlo[(N+1),1:len]       # Mean points in each area after resampling
areas@data$conf.upper <- monte.carlo[(N+2),1:len] # Lower confidence interval
areas@data$conf.lower <- monte.carlo[(N+3),1:len] # Upper confidence interval
