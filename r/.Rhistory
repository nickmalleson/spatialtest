version()
v()
Version()
version
d <- read.csv('mapping/writing/sttc/twitter_data/2/satscan/tweets_cases-hours.csv')
d
names(d)
hist(d$Count)
hist(d$Count, breaks="Scott")
load("/Volumes/a204/geonsm/mapping/writing/ambient_crime3/ambientcrime3.RData")
cpd.ambient.aggregate <- aggregate(Total ~ Grid.ID, data=cpd.ambient, mean)
cpd <- merge(x=lon.grid, y=cpd.ambient.aggregate, by.x="ID", by.y="Grid.ID", all.x=TRUE)
plot(cpd)
save.image("/Volumes/a204/geonsm/mapping/writing/ambient_crime3/ambientcrime3.RData")
head(cpd, 10)
cpd.ambient.aggregate <- aggregate(Total ~ Grid.ID, data=cpd.ambient, mean)
cpd <- merge(x=lon.grid, y=cpd.ambient.aggregate, by.x="ID", by.y="Grid.ID", all.x=TRUE)
save.image("/Volumes/a204/geonsm/mapping/writing/ambient_crime3/ambientcrime3.RData")
plotGoogleMaps(lon.wz, zcol=lon.wz@data$people,filename="temp.html",layerName="Usual Workplace Population", fillOpacity=0.4,strokeWeight=0,mapTypeId="ROADMAP")
library(plotGoogleMaps)
plotGoogleMaps(lon.wz, zcol=lon.wz@data$people,filename="temp.html",layerName="Usual Workplace Population", fillOpacity=0.4,strokeWeight=0,mapTypeId="ROADMAP")
?plotGoogleMaps
plotGoogleMaps(lon.wz, zcol="people" ,filename="temp.html",layerName="Usual Workplace Population", fillOpacity=0.4,strokeWeight=0,mapTypeId="ROADMAP")
library(tmap)
install.packages(tmap)
install.packages("tmap")
vignette(package = "tmap") # available vignettes in tmap
vignette("tmap-nutshell")
qtm(shp = cpd, fill = "people", fill.palette = "-Blues")
library(tmap)
qtm(shp = cpd, fill = "people", fill.palette = "-Blues")
/Crime da
setwd('~/research_not_syncd/git_projects/spatialtest/r/')
library(GISTools)
library(rgeos)    # For things like gIntersects
library(rgdal)    # For reading shapefiles
#library(plyr)     # For counting and aggregating
#library(tmap)     # For thematic maps
#library(classInt) # Jenks natural breaks
#library(png)      # For loading pngs after they have been written
#library(grid)     # so that they can be embedded in knitted documents
#library(spdep)    # For doing the spatial regression, contiguity matrices, etc.
#library(GWmodel)  # For geographically weighted regression (GWR)
#library(MASS)     # For stepwise regression (stepAIC())
#library(pander)   # For printing tables nicely
#library(MVN)      # For testing for multivariate normality (MVN)
# Read some data
areas <- readOGR(ds = "./data/", layer="areas")
base.data <- readOGR(ds = "./data/", layer="points1")
test.data <- readOGR(ds = "./data/", layer="points2")
plot(areas)
plot(base.data, col="blue",add=T)
plot(test.data, col="red",add=T)
andresen.spatial.test <- function(base, test, areas, debug=TRUE, monte.carlo=100) {
# Create a list to store the number of points in each area at every monte-carlo iteration
areas$NumBasePts <- list()
areas$NumTestPts <- list()
# Count the number of base and test features in each area.
# Run the Monte-Carlo simulation (sample points and count the number in each area)
# For each area, calculate the percentage of points in it at every Monte-Carlo iteration
# Rank the areas by the percentages in ascending order and remove outliers
# Calculate the S-index for each area
# Calculate global S value
# Return results
}
andresen.spatial.test(base.data, test.data, areas)
andresen.spatial.test <- function(base, test, areas, debug=TRUE, monte.carlo=100) {
# Create a list to store the number of points in each area at every monte-carlo iteration
areas$NumBasePts <- list()
areas$NumTestPts <- list()
# Count the number of base and test features in each area.
# Run the Monte-Carlo simulation (sample points and count the number in each area)
# For each area, calculate the percentage of points in it at every Monte-Carlo iteration
# Rank the areas by the percentages in ascending order and remove outliers
# Calculate the S-index for each area
# Calculate global S value
# Return results
return(areas)
}
andresen.spatial.test(base.data, test.data, areas)
t <- andresen.spatial.test(base.data, test.data, areas)
andresen.spatial.test <- function(base, test, areas, debug=TRUE, monte.carlo=100) {
# Create a list to store the number of points in each area at every monte-carlo iteration
areas$NumBasePts <- list()
areas$NumTestPts <- list()
# Count the number of base and test features in each area.
# Run the Monte-Carlo simulation (sample points and count the number in each area)
# For each area, calculate the percentage of points in it at every Monte-Carlo iteration
# Rank the areas by the percentages in ascending order and remove outliers
# Calculate the S-index for each area
# Calculate global S value
# Return results
return(areas)
}
t <- andresen.spatial.test(base.data, test.data, areas)
andresen.spatial.test <- function(base, test, areas, debug=TRUE, monte.carlo=100) {
# Create a list to store the number of points in each area at every monte-carlo iteration
areas$NumBasePts <- list()
areas$NumTestPts <- list()
# Count the number of base and test features in each area.
# Run the Monte-Carlo simulation (sample points and count the number in each area)
# For each area, calculate the percentage of points in it at every Monte-Carlo iteration
# Rank the areas by the percentages in ascending order and remove outliers
# Calculate the S-index for each area
# Calculate global S value
# Return results
return(areas)
}
t <- andresen.spatial.test(base=base.data, test=test.data, areas=areas)
